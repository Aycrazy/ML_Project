{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%run Pipeline//upload_and_vizualize \n",
    "%run Pipeline//classify_and_evaluate \n",
    "%run Pipeline//aux\n",
    "%run Pipeline//ULAB_ML_Pipeline\n",
    "%run processing\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import sys\n",
    "import random\n",
    "import sklearn as sk \n",
    "import json \n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from time import time\n",
    "from sklearn import svm, ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, RandomizedLogisticRegression\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import *\n",
    "import csv\n",
    "from errno import EEXIST\n",
    "from os import makedirs,path\n",
    "from datetime import datetime as dr\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import pylab as pl\n",
    "from upload_and_vizualize import camel_to_snake\n",
    "from datetime import datetime as dt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### SHOULD BE WITHIN CLEANING.PY ###\n",
    "\n",
    "## FROM PREPROCESSING ##\n",
    "\n",
    "#interest_var = ['PGM_SYS_ID','ACTIVITY_ID','AGENCY_TYPE_DESC','STATE_CODE','AIR_LCON_CODE','COMP_DETERMINATION_UID','ENF_RESPONSE_POLICY_CODE','PROGRAM_CODES']\n",
    "def replace_with_value(data_file, variables, values):\n",
    "    '''\n",
    "    '''\n",
    "    for variable in variables:\n",
    "        value = values[variables.index(variable)]\n",
    "        data_file[variable] = data_file[variable].fillna(value)\n",
    "\n",
    "def convert_to_datetime(series_row, date_format):\n",
    "    if str(series_row) == 'nan':\n",
    "        return float('nan')\n",
    "    return dt.strptime(series_row, date_format)\n",
    "\n",
    "def convert_to_year(series_row):\n",
    "    if str(series_row) == 'NaT' or str(series_row)== 'nan':\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return str(series_row.year)\n",
    "\n",
    "def to_date_time(df, date_format, date_col):\n",
    "    #add datetime column\n",
    "    df[date_col+'_datetime'] = df[date_col].apply(convert_to_datetime, date_format=date_format)\n",
    "    df[date_col+'_year'] = df[date_col+'_datetime'].apply(convert_to_year)\n",
    "\n",
    "    #return df\n",
    "\n",
    "def convert_to_month(series_row):\n",
    "    if str(series_row) == 'NaT' or str(series_row)== 'nan':\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return str(series_row.month)\n",
    "\n",
    "def get_month_year_col(df, date_column, date_format):\n",
    "    df[date_column+'_datetime'] = df[date_column].apply(convert_to_datetime, date_format=date_format)\n",
    "    df[date_column+'_month'] = df[date_column+'_datetime'].apply(convert_to_month)\n",
    "    df[date_column+'_year'] = df[date_column+'_datetime'].apply(convert_to_year)\n",
    "    return df\n",
    "\n",
    "def filter_date(df, date_format, date_col, start=None, end=None):\n",
    "    to_date_time(df, date_format, date_col)\n",
    "    \n",
    "    \n",
    "    if start:\n",
    "        timestart = dt.strptime(start,\"%Y/%m/%d\")\n",
    "        #print(start)\n",
    "        df = df[df[date_col+'_datetime'] >= timestart ]\n",
    "        #print(df.head())\n",
    "    if end:\n",
    "        timeend = dt.strptime(end,\"%Y/%m/%d\")\n",
    "        #print(end)\n",
    "        df = df[df[date_col+'_datetime'] <= timeend ]\n",
    "        #print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_col(df, fac_id, features, date_col):\n",
    "    #filter needed\n",
    "    df = df[[fac_id] + [date_col+'_datetime'] + [date_col+'_year'] + features]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## FROM PREPROCESSING, MODIFIED ##\n",
    "def add_dummy(df, variable_list, sep_char = None, drop_one=False, drop_original=False):\n",
    "    '''\n",
    "    Input: \n",
    "        - df: pandas dataframe\n",
    "        - variable_list: a list of variables to dummitize\n",
    "        - drop_one: whether to drop first dummy\n",
    "        - drop_original: whether to drop original categorical variable\n",
    "    Output: dataframe with tht dummy variables added\n",
    "    '''\n",
    "    for variable in variable_list:\n",
    "        if sep_char:\n",
    "            df_dummy = df[variable].str.get_dummies(sep=sep_char)\n",
    "            df_dummy.columns = [variable+ '_' +str(col) for col in df_dummy.columns]\n",
    "\n",
    "        else:\n",
    "            df_dummy = pd.get_dummies(df[variable], drop_first=drop_one, prefix = variable)\n",
    "        \n",
    "        df = pd.concat([df, df_dummy], axis=1)\n",
    "        if drop_original:\n",
    "            df = df.drop(variable, 1)\n",
    "    return (df, df_dummy.columns)\n",
    "\n",
    "def aggr_dummy_cols(df, final_df, colnames, mode = None):\n",
    "    for col in colnames:\n",
    "        \n",
    "        cross = pd.crosstab(df['id_+_date'], columns=df[col])\n",
    "        \n",
    "        if mode == 'cat':\n",
    "            cross.columns = [cross.columns.name+ '_' +str(col) for col in cross.columns]\n",
    "        \n",
    "        elif mode == 'dum':\n",
    "            cross = cross.drop(0, axis = 1)\n",
    "            cross.columns = [cross.columns.name for col in cross.columns]\n",
    "            \n",
    "        #SIMPLIFY THIS!!\n",
    "        elif mode == 'bim':\n",
    "            cross = cross.drop('N', axis = 1)\n",
    "            cross.columns = [cross.columns.name for col in cross.columns]\n",
    "        \n",
    "        #ADD PROCESSING CONT_VAR\n",
    "        \n",
    "        else:\n",
    "            cross.columns = [cross.columns.name for col in cross.columns]\n",
    "            \n",
    "        cross.columns.name = None\n",
    "        cross.reset_index(inplace=True)\n",
    "        \n",
    "        \n",
    "        if final_df.empty:\n",
    "            final_df = final_df.append(cross)\n",
    "        else:\n",
    "            final_df = pd.merge(final_df, cross, how = 'left', on = 'id_+_date')\n",
    "            \n",
    "    return final_df\n",
    "\n",
    "## FROM ULAB PIPELINE ##\n",
    "\n",
    "def generate_continous_variable(data_file, variable_list):\n",
    "    '''\n",
    "    function that can take a categorical variable and create \n",
    "    binary variables from it\n",
    "    '''\n",
    "    for variable in variable_list:\n",
    "        list_values = list(data_file.groupby(variable).groups.keys())\n",
    "        for i,value in enumerate(list_values):\n",
    "            data_file[variable].replace(value,i)\n",
    "\n",
    "    return data_file \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FACILITY = 'data/ICIS-AIR_downloads/ICIS-AIR_FACILITIES.csv'\n",
    "VIOLATION = 'data/ICIS-AIR_downloads/ICIS-AIR_VIOLATION_HISTORY.csv'\n",
    "INSPECTION = 'data/ICIS-AIR_downloads/ICIS-AIR_FCES_PCES.csv'\n",
    "FORMALACT = 'data/ICIS-AIR_downloads/ICIS-AIR_FORMAL_ACTIONS.csv'\n",
    "INFORMALACT = 'data/ICIS-AIR_downloads/ICIS-AIR_INFORMAL_ACTIONS.csv'\n",
    "STACKTEST = 'data/ICIS-AIR_downloads/ICIS-AIR_STACK_TESTS.csv'\n",
    "TITLEV = 'data/ICIS-AIR_downloads/ICIS-AIR_TITLEV_CERTS.csv'\n",
    "\n",
    "#facilities = read_file('ICIS-AIR_downloads/ICIS-AIR_FACILITIES.csv')\n",
    "#pd_violations = read_file('ICIS-AIR_downloads/ICIS-AIR_VIOLATION_HISTORY.csv')\n",
    "#pd_inspection = read_file('ICIS-AIR_downloads/ICIS-AIR_FCES_PCES.csv')\n",
    "#pd_formalact = read_file('ICIS-AIR_downloads/ICIS-AIR_FORMAL_ACTIONS.csv')\n",
    "#pd_informalact = read_file('ICIS-AIR_downloads/ICIS-AIR_INFORMAL_ACTIONS.csv')\n",
    "#pd_stacktest = read_file('ICIS-AIR_downloads/ICIS-AIR_STACK_TESTS.csv')\n",
    "#pd_titlev = read_file('ICIS-AIR_downloads/ICIS-AIR_TITLEV_CERTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## CONFIG DATA ##\n",
    "START_DATE= '2007/01/01'\n",
    "END_DATE = '2016/12/31'\n",
    "fac_id = 'PGM_SYS_ID'\n",
    "df_dict ={'violation': {#'data_file': 'data/ICIS-AIR_downloads/ICIS-AIR_VIOLATION_HISTORY.csv',\n",
    "                        'interest_var': ['AGENCY_TYPE_DESC','AIR_LCON_CODE','ENF_RESPONSE_POLICY_CODE','POLLUTANT_CODES','PROGRAM_CODES','HPV_RESOLVED_DATE'],\n",
    "                         'date_col': 'HPV_DAYZERO_DATE',\n",
    "                       'date_format':'%m-%d-%Y'},\n",
    "          \n",
    "           'inspection': {'interest_var': ['STATE_EPA_FLAG','COMP_MONITOR_TYPE_CODE','PROGRAM_CODES'],\n",
    "                         'date_col': 'ACTUAL_END_DATE',\n",
    "                         'date_format':'%m-%d-%Y'},\n",
    "          \n",
    "           'stacktest': {'interest_var':['COMP_MONITOR_TYPE_CODE','POLLUTANT_CODES','AIR_STACK_TEST_STATUS_CODE'],\n",
    "                        'date_col': 'ACTUAL_END_DATE',\n",
    "                        'date_format':'%m/%d/%Y'},\n",
    "          \n",
    "           'titlev':{'interest_var':['COMP_MONITOR_TYPE_CODE','FACILITY_RPT_DEVIATION_FLAG'],\n",
    "                        'date_col': 'ACTUAL_END_DATE',\n",
    "                    'date_format':'%m/%d/%Y'},\n",
    "          \n",
    "           'formalact':{'interest_var':['ENF_TYPE_CODE','PENALTY_AMOUNT'],\n",
    "                        'date_col': 'SETTLEMENT_ENTERED_DATE',\n",
    "                       'date_format':'%m/%d/%Y'},\n",
    "          \n",
    "           'informalact':{'interest_var':['ENF_TYPE_CODE'],\n",
    "                        'date_col': 'ACHIEVED_DATE',\n",
    "                         'date_format':'%m/%d/%Y'}}\n",
    "\n",
    "#general_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HPV_DAYZERO_DATE'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['violation']['date_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## READ INITIAL FILTERED FILE ##\n",
    "violation, inspection, titlev, stacktest, formalact, informalact = general_read_file(df_dict, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "violation = read_file('data/ICIS-AIR_downloads/ICIS-AIR_VIOLATION_HISTORY.csv')\n",
    "inspection = read_file('data/ICIS-AIR_downloads/ICIS-AIR_FCES_PCES.csv')\n",
    "date_types = ['year']\n",
    "date_format = df_dict['violation']['date_format']\n",
    "date_col = [df_dict['violation']['date_col']]\n",
    "#print(date_format,date_col)\n",
    "get_occupied_frame(violation,date_col,date_format,date_types)\n",
    "\n",
    "date_col = [df_dict['inspection']['date_col']]\n",
    "get_occupied_frame(inspection,date_col,date_format,date_types)\n",
    "\n",
    "fce = inspection[['PGM_SYS_ID','STATE_EPA_FLAG','COMP_MONITOR_TYPE_CODE','PROGRAM_CODES','ACTUAL_END_DATE','ACTUAL_END_DATE_year','ACTUAL_END_DATE_datetime']]\n",
    "violation = violation[['PGM_SYS_ID','AGENCY_TYPE_DESC','AIR_LCON_CODE','ENF_RESPONSE_POLICY_CODE','POLLUTANT_CODES','PROGRAM_CODES','HPV_DAYZERO_DATE','HPV_DAYZERO_DATE_year','HPV_DAYZERO_DATE_datetime']]\n",
    "violation = violation[violation.ENF_RESPONSE_POLICY_CODE == 'HPV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### FAILED LABEL #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def change_to_zero(series_row):\n",
    "    if type(series_row) != str:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def change_to_zero_float(series_row):\n",
    "    if series_row >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def generate_label(violhist, fce, start_year, end_year):\n",
    "    #removing FRVs\n",
    "    #violhist = violhist[violhist.ENF_RESPONSE_POLICY_CODE != 'FRV']\n",
    "    violhist = violhist[violhist['HPV_DAYZERO_DATE_year'] >= start_year]\n",
    "    violhist = violhist[violhist['HPV_DAYZERO_DATE_year'] <= end_year]\n",
    "    \n",
    "    #for fce\n",
    "    fce = fce[fce['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    fce = fce[fce['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    merged_hpv_fce = pd.merge(violhist, fce, how='right', left_on=['PGM_SYS_ID', 'HPV_DAYZERO_DATE'], right_on=['PGM_SYS_ID','ACTUAL_END_DATE'])\n",
    "    \n",
    "    merged_hpv_fce['Outcome'] = merged_hpv_fce.ENF_RESPONSE_POLICY_CODE.apply(change_to_zero)\n",
    "    #finding 0's\n",
    "    '''\n",
    "    non_viol = merged_hpv_fce[merged_hpv_fce.HPV_DAYZERO_DATE.isnull()]\n",
    "    non_viol['is_violation'] = 0\n",
    "    '''\n",
    "    output = merged_hpv_fce.groupby(['PGM_SYS_ID', 'ACTUAL_END_DATE_year']).sum().reset_index()\n",
    "    \n",
    "    output.Outcome = output.Outcome.apply(change_to_zero_float)\n",
    "    #non_viol = merged_hpv_fce\n",
    "    #non_viol.HPV_DAYZERO_DATE.fillna(0, inplace=True)\n",
    "    #non_viol = non_viol.filter(['PGM_SYS_ID', 'ACTUAL_END_DATE', 'is_violation', 'ACTUAL_END_DATE_year'], axis = 1)\n",
    "    #***non_viol = non_viol[['PGM_SYS_ID', 'ACTUAL_END_DATE', 'ACTUAL_END_DATE_year']]\n",
    "    #non_viol.rename(columns={'ACTUAL_END_DATE': 'HPV_DAYZERO_DATE', 'ACTUAL_END_DATE_year': \"HPV_DAYZERO_DATE_year\" }, inplace=True)\n",
    "    #finding 1's\n",
    "    #viol = merged_hpv_fce[merged_hpv_fce.HPV_DAYZERO_DATE.notnull()]\n",
    "    #***viol = merged_hpv_fce\n",
    "    #***viol.HPV_DAYZERO_DATE.fillna(0, inplace=True)\n",
    "    #viol['is_violation'] = 1\n",
    "    #viol = viol.filter(['PGM_SYS_ID', 'HPV_DAYZERO_DATE', 'is_violation', 'HPV_DAYZERO_DATE_year'], axis = 1)\n",
    "    #***viol = viol[['PGM_SYS_ID', 'HPV_DAYZERO_DATE', 'HPV_DAYZERO_DATE_year']]\n",
    "    #***output = pd.concat([viol,non_viol])\n",
    "    #output['id_+_date'] = output.PGM_SYS_ID +'_'+ output.HPV_DAYZERO_DATE_year\n",
    "    #output.drop(['PGM_SYS_ID', 'HPV_DAYZERO_DATE', 'HPV_DAYZERO_DATE_year'], axis = 1, inplace = True)\n",
    "    #output = output.groupby('id_+_date').size().reset_index()\n",
    "    #output.rename(columns = {0:\"outcome\"}, inplace = True)\n",
    "    #return output\n",
    "    return output.drop('ACTIVITY_ID',axis=1), merged_hpv_fce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date = '2007'\n",
    "end_date = '2016'\n",
    "out, merged = generate_label(violation, inspection, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PGM_SYS_ID</th>\n",
       "      <th>ACTUAL_END_DATE_year</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>020000000003605590</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>020000003400500100</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>020000003400546143</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>020000003400710001</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>020000003400900010</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PGM_SYS_ID ACTUAL_END_DATE_year  Outcome\n",
       "0  020000000003605590                 2015        0\n",
       "1  020000003400500100                 2014        0\n",
       "2  020000003400546143                 2010        0\n",
       "3  020000003400710001                 2015        0\n",
       "4  020000003400900010                 2013        0"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    338010\n",
       "1      1984\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    683079\n",
       "1      4784\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def general_read_file(df_dict, start_date, end_date):\n",
    "    #df = pd.DataFrame()\n",
    "    \n",
    "    violation = []\n",
    "    inspection = []\n",
    "    stacktest = []\n",
    "    titlev = []\n",
    "    formalact = []\n",
    "    informalact = []\n",
    "    \n",
    "    for table, var in df_dict.items():\n",
    "        date_col = var['date_col']\n",
    "        features = var['interest_var']\n",
    "        DATE_FORMAT = var['date_format']\n",
    "        \n",
    "        if table == 'violation':\n",
    "            violation = read_file(VIOLATION)\n",
    "            violation = violation[violation['ENF_RESPONSE_POLICY_CODE'] != 'FRV']\n",
    "            \n",
    "            violation = filter_date(violation, DATE_FORMAT, date_col, start=start_date, end=end_date)\n",
    "            violation = filter_col(violation, fac_id, features, date_col)\n",
    "        \n",
    "        elif table == 'inspection':\n",
    "            inspection = read_file(INSPECTION)\n",
    "            \n",
    "            inspection = filter_date(inspection, DATE_FORMAT, date_col, start=start_date, end=end_date)\n",
    "            inspection = filter_col(inspection, fac_id, features, date_col)\n",
    "        \n",
    "        elif table == 'titlev':\n",
    "            titlev = read_file(TITLEV)\n",
    "            \n",
    "            titlev = filter_date(titlev, DATE_FORMAT, date_col, start=start_date, end=end_date)\n",
    "            titlev = filter_col(titlev, fac_id, features, date_col)\n",
    "        \n",
    "        elif table == 'stacktest':\n",
    "            stacktest = read_file(STACKTEST)\n",
    "            \n",
    "            stacktest = filter_date(stacktest, DATE_FORMAT, date_col, start=start_date, end=end_date)\n",
    "            stacktest = filter_col(stacktest, fac_id, features, date_col)\n",
    "        \n",
    "        elif table == 'formalact':\n",
    "            formalact = read_file(FORMALACT)\n",
    "            \n",
    "            formalact = filter_date(formalact, DATE_FORMAT, date_col, start=start_date, end=end_date)\n",
    "            formalact = filter_col(formalact, fac_id, features, date_col)\n",
    "        \n",
    "        elif table == 'informalact':\n",
    "            informalact = read_file(INFORMALACT)\n",
    "            \n",
    "            informalact = filter_date(informalact, DATE_FORMAT, date_col, start=start_date, end=end_date)\n",
    "            informalact = filter_col(informalact, fac_id, features, date_col)\n",
    "        \n",
    "    return (violation, inspection, titlev, stacktest, formalact, informalact)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def process_violation(violation_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    outcome = ['ENF_RESPONSE_POLICY_CODE']\n",
    "    cat_var = ['AGENCY_TYPE_DESC', 'AIR_LCON_CODE']\n",
    "    dum_var = ['PROGRAM_CODES', 'POLLUTANT_CODES']\n",
    "    \n",
    "    df = violation_df[violation_df['HPV_DAYZERO_DATE_year'] >= start_year]\n",
    "    df = df[df['HPV_DAYZERO_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 'None' (string) --> Making 'None' it's own category\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize \n",
    "    df, colnames_out = add_dummy(df, outcome, drop_original = True)\n",
    "    df, colnames_dum = add_dummy(df, dum_var, sep_char = ' ')\n",
    "    \n",
    "    #return df\n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.HPV_DAYZERO_DATE_year\n",
    "            \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    final_df = aggr_dummy_cols(df, final_df, colnames_dum, 'dum')\n",
    "    final_df = aggr_dummy_cols(df, final_df, colnames_out)\n",
    "    \n",
    "    '''re_separate = r'(.[^_]*)_(.*)'\n",
    "    sep = lambda x: pd.Series([i for i in re.split(re_separate,x)])\n",
    "    final_id_year = final_df['id_+_date'].apply(sep)\n",
    "    final_df =pd.concat([final_id_year.rename(columns={1:'PGM_SYS_ID',2:'HPV_DAYZERO_DATE_year'}), final_df], axis=1)\n",
    "    final_df.drop([0, 3, 'id_+_date'], axis = 1, inplace = True)\n",
    "'''\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_inspection(inspection_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    cat_var = ['STATE_EPA_FLAG','COMP_MONITOR_TYPE_CODE']\n",
    "    dum_var = ['PROGRAM_CODES']\n",
    "    \n",
    "    df = inspection_df[inspection_df['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    df = df[df['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 'None' (string) --> Making 'None' it's own category\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize \n",
    "    df, colnames_dum = add_dummy(df, dum_var, sep_char = ',') \n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.ACTUAL_END_DATE_year\n",
    "            \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    final_df = aggr_dummy_cols(df, final_df, colnames_dum, 'dum')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_titlev(titlev_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    cat_var = ['COMP_MONITOR_TYPE_CODE']\n",
    "    bim_var = ['FACILITY_RPT_DEVIATION_FLAG']\n",
    "    \n",
    "    df = titlev_df[titlev_df['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    df = df[df['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 0 (string) --> THIS ONLY APPLIES TO THE BIM_VAR!!\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['N']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize \n",
    "    df = generate_continous_variable(df, bim_var)\n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.ACTUAL_END_DATE_year\n",
    "    \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    final_df = aggr_dummy_cols(df, final_df, bim_var, 'bim')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_stacktest(stacktest_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    #I HAVEN'T PUT IN POLLUTANT_CODES\n",
    "    cat_var = ['AIR_STACK_TEST_STATUS_CODE', 'COMP_MONITOR_TYPE_CODE']\n",
    "    \n",
    "    df = stacktest_df[stacktest_df['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    df = df[df['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 0 (string) --> THIS ONLY APPLIES TO THE BIM_VAR!!\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize --> Not really needed here \n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.ACTUAL_END_DATE_year\n",
    "    \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def process_formalact(formalact_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    cat_var = ['ENF_TYPE_CODE']\n",
    "    cont_var = ['PENALTY_AMOUNT']\n",
    "    \n",
    "    df = formalact_df[formalact_df['SETTLEMENT_ENTERED_DATE_year'] >= start_year]\n",
    "    df = df[df['SETTLEMENT_ENTERED_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 0 (string) --> No need for this but ill leave it here\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize --> Not really needed here \n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.SETTLEMENT_ENTERED_DATE_year\n",
    "    \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    #NEED TO ADD THE SUM of the amount!\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_informalact(informalact_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    cat_var = ['ENF_TYPE_CODE']\n",
    "    \n",
    "    df = informalact_df[informalact_df['ACHIEVED_DATE_year'] >= start_year]\n",
    "    df = df[df['ACHIEVED_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 0 (string) --> No need for this but ill leave it here\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize --> Not really needed here \n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.ACHIEVED_DATE_year\n",
    "    \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def process_noninspectHPV(violhist, fce, start_year, end_year):\n",
    "    #removing FRVs\n",
    "    violhist = violhist[violhist.ENF_RESPONSE_POLICY_CODE != 'FRV']\n",
    "    violhist = violhist[violhist['HPV_DAYZERO_DATE_year'] >= start_year]\n",
    "    violhist = violhist[violhist['HPV_DAYZERO_DATE_year'] <= end_year]\n",
    "    \n",
    "    #for fce\n",
    "    fce = fce[fce['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    fce = fce[fce['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    #Steps before merge\n",
    "    violhist['year'] = violhist['HPV_DAYZERO_DATE_year']\n",
    "    merged_hpv_fce = pd.merge(violhist, fce, how='left', left_on=['PGM_SYS_ID', 'HPV_DAYZERO_DATE'], right_on=['PGM_SYS_ID','ACTUAL_END_DATE'])\n",
    "    # Find violations that resulted from something other than an inspection \n",
    "    viol_by_other = merged_hpv_fce\n",
    "    viol_by_other.COMP_MONITOR_TYPE_CODE.fillna(0, inplace=True)   #this will be NaN because it was not inspected\n",
    "    viol_by_other = viol_by_other[viol_by_other['COMP_MONITOR_TYPE_CODE'] == 0]  #violations not resulting from inspections\n",
    "    # Get the columns needed\n",
    "    viol_by_other = viol_by_other[['PGM_SYS_ID','year']]\n",
    "    viol_other_year = viol_by_other.groupby(['PGM_SYS_ID','year']).size().reset_index() # to get count of HPV by year\n",
    "    violhist2 = violhist[['PGM_SYS_ID','year']]\n",
    "    # Outer merge\n",
    "    merged_viols = pd.merge(violhist2,viol_other_year, how = 'outer', on = ['PGM_SYS_ID','year'])\n",
    "    merged_viols.rename(columns={'year': 'Year', 0:'NonInspection_HPV_Count'}, inplace=True)\n",
    "    merged_viols.NonInspection_HPV_Count.fillna(0, inplace=True)\n",
    "    merged_viols = merged_viols.dropna(axis=0)\n",
    "    merged_viols['id_+_date'] = merged_viols.PGM_SYS_ID +'_'+ merged_viols.Year\n",
    "    merged_viols.drop(['PGM_SYS_ID', 'Year'], axis = 1, inplace = True)\n",
    "    \n",
    "    return merged_viols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### SHOULD BE WITHIN CLEANING.PY ###\n",
    "\n",
    "## FROM PREPROCESSING ##\n",
    "\n",
    "#interest_var = ['PGM_SYS_ID','ACTIVITY_ID','AGENCY_TYPE_DESC','STATE_CODE','AIR_LCON_CODE','COMP_DETERMINATION_UID','ENF_RESPONSE_POLICY_CODE','PROGRAM_CODES']\n",
    "def replace_with_value(data_file, variables, values):\n",
    "    '''\n",
    "    '''\n",
    "    for variable in variables:\n",
    "        value = values[variables.index(variable)]\n",
    "        data_file[variable] = data_file[variable].fillna(value)\n",
    "\n",
    "def convert_to_datetime(series_row, date_format):\n",
    "    if str(series_row) == 'nan':\n",
    "        return float('nan')\n",
    "    return dt.strptime(series_row, date_format)\n",
    "\n",
    "def convert_to_year(series_row):\n",
    "    if str(series_row) == 'NaT' or str(series_row)== 'nan':\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return str(series_row.year)\n",
    "\n",
    "def to_date_time(df, date_format, date_col):\n",
    "    #add datetime column\n",
    "    df[date_col] = df[date_col].apply(convert_to_datetime, date_format=date_format)\n",
    "    df[date_col+'_year'] = df[date_col].apply(convert_to_year)\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_to_month(series_row):\n",
    "    if str(series_row) == 'NaT' or str(series_row)== 'nan':\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return str(series_row.month)\n",
    "\n",
    "def get_month_year_col(df, date_column, date_format):\n",
    "    df[date_column+'_datetime'] = df[date_column].apply(convert_to_datetime, date_format=date_format)\n",
    "    df[date_column+'_month'] = df[date_column+'_datetime'].apply(convert_to_month)\n",
    "    df[date_column+'_year'] = df[date_column+'_datetime'].apply(convert_to_year)\n",
    "    return df\n",
    "\n",
    "def filter_date(df, date_format, date_col, start=None, end=None):\n",
    "    df = to_date_time(df, date_format, date_col)\n",
    "    \n",
    "    \n",
    "    if start:\n",
    "        timestart = dt.strptime(start,\"%Y/%m/%d\")\n",
    "        #print(start)\n",
    "        df = df[df[date_col] >= timestart ]\n",
    "        #print(df.head())\n",
    "    if end:\n",
    "        timeend = dt.strptime(end,\"%Y/%m/%d\")\n",
    "        #print(end)\n",
    "        df = df[df[date_col] <= timeend ]\n",
    "        #print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_col(df, fac_id, features, date_col):\n",
    "    #filter needed\n",
    "    df = df[[fac_id] + [date_col] + [date_col+'_year'] + features]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## FROM PREPROCESSING, MODIFIED ##\n",
    "def add_dummy(df, variable_list, sep_char = None, drop_one=False, drop_original=False):\n",
    "    '''\n",
    "    Input: \n",
    "        - df: pandas dataframe\n",
    "        - variable_list: a list of variables to dummitize\n",
    "        - drop_one: whether to drop first dummy\n",
    "        - drop_original: whether to drop original categorical variable\n",
    "    Output: dataframe with tht dummy variables added\n",
    "    '''\n",
    "    for variable in variable_list:\n",
    "        if sep_char:\n",
    "            df_dummy = df[variable].str.get_dummies(sep=sep_char)\n",
    "            df_dummy.columns = [variable+ '_' +str(col) for col in df_dummy.columns]\n",
    "\n",
    "        else:\n",
    "            df_dummy = pd.get_dummies(df[variable], drop_first=drop_one, prefix = variable)\n",
    "        \n",
    "        df = pd.concat([df, df_dummy], axis=1)\n",
    "        if drop_original:\n",
    "            df = df.drop(variable, 1)\n",
    "    return (df, df_dummy.columns)\n",
    "\n",
    "def aggr_dummy_cols(df, final_df, colnames, mode = None):\n",
    "    for col in colnames:\n",
    "        \n",
    "        cross = pd.crosstab(df['id_+_date'], columns=df[col])\n",
    "        \n",
    "        if mode == 'cat':\n",
    "            cross.columns = [cross.columns.name+ '_' +str(col) for col in cross.columns]\n",
    "        \n",
    "        elif mode == 'dum':\n",
    "            cross = cross.drop(0, axis = 1)\n",
    "            cross.columns = [cross.columns.name for col in cross.columns]\n",
    "            \n",
    "        #SIMPLIFY THIS!!\n",
    "        elif mode == 'bim':\n",
    "            cross = cross.drop('N', axis = 1)\n",
    "            cross.columns = [cross.columns.name for col in cross.columns]\n",
    "        \n",
    "        #ADD PROCESSING CONT_VAR\n",
    "        \n",
    "        else:\n",
    "            cross.columns = [cross.columns.name for col in cross.columns]\n",
    "            \n",
    "        cross.columns.name = None\n",
    "        cross.reset_index(inplace=True)\n",
    "        \n",
    "        \n",
    "        if final_df.empty:\n",
    "            final_df = final_df.append(cross)\n",
    "        else:\n",
    "            final_df = pd.merge(final_df, cross, how = 'left', on = 'id_+_date')\n",
    "            \n",
    "    return final_df\n",
    "\n",
    "## FROM ULAB PIPELINE ##\n",
    "\n",
    "def generate_continous_variable(data_file, variable_list):\n",
    "    '''\n",
    "    function that can take a categorical variable and create \n",
    "    binary variables from it\n",
    "    '''\n",
    "    for variable in variable_list:\n",
    "        list_values = list(data_file.groupby(variable).groups.keys())\n",
    "        for i,value in enumerate(list_values):\n",
    "            data_file[variable].replace(value,i)\n",
    "\n",
    "    return data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_features(start_date, end_date):\n",
    "    #violation, inspection, titlev, stacktest, formalact, informalact = general_read_file(df_dict, START_DATE, END_DATE)\n",
    "    \n",
    "    violation_df = process_violation(violation, start_date, end_date)\n",
    "    inspection_df = process_titlev(titlev, start_date, end_date)\n",
    "    stacktest_df = process_stacktest(stacktest, start_date, end_date)\n",
    "    formalact_df = process_formalact(formalact, start_date, end_date)\n",
    "    informalact_df = process_informalact(informalact, start_date, end_date)\n",
    "    noninspectHPV_df = process_noninspectHPV(violation, inspection, start_date, end_date)\n",
    "    \n",
    "    final_df = pd.merge(inspection_df, violation_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "    final_df = pd.merge(final_df, stacktest_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "    final_df = pd.merge(final_df, formalact_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "    final_df = pd.merge(final_df, informalact_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "    final_df = pd.merge(final_df, noninspectHPV_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "\n",
    "    re_separate = r'(.[^_]*)_(.*)'\n",
    "    sep = lambda x: pd.Series([i for i in re.split(re_separate,x)])\n",
    "    final_id_year = final_df['id_+_date'].apply(sep)\n",
    "    final_df =pd.concat([final_id_year.rename(columns={1:'PGM_SYS_ID',2:'HPV_DAYZERO_DATE_year'}), final_df], axis=1)\n",
    "    final_df.drop([0, 3, 'id_+_date'], axis = 1, inplace = True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56246, 173)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for feature generation\n",
    "start_date = '2009'\n",
    "end_date = '2012'\n",
    "feat = generate_features(start_date, end_date)\n",
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
