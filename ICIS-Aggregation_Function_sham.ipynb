{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "ERROR:root:File `'Pipeline//aux.py'` not found.\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (ULAB_ML_Pipeline.py, line 204)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"C:\\Users\\mohan\\OneDrive\\Documents\\CS\\ML\\ML_Project\\Pipeline\\ULAB_ML_Pipeline.py\"\u001b[0;36m, line \u001b[0;32m204\u001b[0m\n\u001b[0;31m    plt.savefig(corr_dir\"/\"+x+\"-\"+y+\"_correlation.png\", bbox_inches = \"tight\")\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%run Pipeline//upload_and_vizualize \n",
    "%run Pipeline//classify_and_evaluate \n",
    "%run Pipeline//aux\n",
    "%run Pipeline//ULAB_ML_Pipeline\n",
    "%run processing\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import sys\n",
    "import random\n",
    "import sklearn as sk \n",
    "import json \n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from time import time\n",
    "from sklearn import svm, ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, RandomizedLogisticRegression\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import *\n",
    "import csv\n",
    "from errno import EEXIST\n",
    "from os import makedirs,path\n",
    "from datetime import datetime as dr\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import pylab as pl\n",
    "from upload_and_vizualize import camel_to_snake\n",
    "from datetime import datetime as dt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def general_read_file(df_dict, table_name, start_date, end_date):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    table = df_dict[table_name]\n",
    "    data_file = table['data_file']\n",
    "    date_col = table['date_col']\n",
    "    DATE_FORMAT = table['date_format']\n",
    "    features = table['interest_var']\n",
    "    \n",
    "    df = read_file(data_file)\n",
    "    \n",
    "    if table == 'violation':\n",
    "        df = violation[violation['ENF_RESPONSE_POLICY_CODE'] != 'FRV']\n",
    "            \n",
    "    df = filter_date(df, DATE_FORMAT, date_col, start=start_date, end=end_date)\n",
    "    df = filter_col(df, fac_id, features, date_col)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_violation(violation_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    outcome = ['ENF_RESPONSE_POLICY_CODE']\n",
    "    cat_var = ['AGENCY_TYPE_DESC', 'AIR_LCON_CODE']\n",
    "    dum_var = ['PROGRAM_CODES', 'POLLUTANT_CODES']\n",
    "    \n",
    "    df = violation_df[violation_df['HPV_DAYZERO_DATE_year'] >= start_year]\n",
    "    df = df[df['HPV_DAYZERO_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 'None' (string) --> Making 'None' it's own category\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize \n",
    "    df, colnames_out = add_dummy(df, outcome, drop_original = True)\n",
    "    df, colnames_dum = add_dummy(df, dum_var, sep_char = ' ')\n",
    "    \n",
    "    #return df\n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.HPV_DAYZERO_DATE_year\n",
    "            \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    final_df = aggr_dummy_cols(df, final_df, colnames_dum, 'dum')\n",
    "    final_df = aggr_dummy_cols(df, final_df, colnames_out)\n",
    "    \n",
    "    '''re_separate = r'(.[^_]*)_(.*)'\n",
    "    sep = lambda x: pd.Series([i for i in re.split(re_separate,x)])\n",
    "    final_id_year = final_df['id_+_date'].apply(sep)\n",
    "    final_df =pd.concat([final_id_year.rename(columns={1:'PGM_SYS_ID',2:'HPV_DAYZERO_DATE_year'}), final_df], axis=1)\n",
    "    final_df.drop([0, 3, 'id_+_date'], axis = 1, inplace = True)\n",
    "'''\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_inspection(inspection_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    cat_var = ['STATE_EPA_FLAG','COMP_MONITOR_TYPE_CODE']\n",
    "    dum_var = ['PROGRAM_CODES']\n",
    "    \n",
    "    df = inspection_df[inspection_df['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    df = df[df['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 'None' (string) --> Making 'None' it's own category\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize \n",
    "    df, colnames_dum = add_dummy(df, dum_var, sep_char = ',') \n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.ACTUAL_END_DATE_year\n",
    "            \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    final_df = aggr_dummy_cols(df, final_df, colnames_dum, 'dum')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_titlev(titlev_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    cat_var = ['COMP_MONITOR_TYPE_CODE']\n",
    "    bim_var = ['FACILITY_RPT_DEVIATION_FLAG']\n",
    "    \n",
    "    df = titlev_df[titlev_df['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    df = df[df['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 0 (string) --> THIS ONLY APPLIES TO THE BIM_VAR!!\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['N']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize \n",
    "    df = generate_continous_variable(df, bim_var)\n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.ACTUAL_END_DATE_year\n",
    "    \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    final_df = aggr_dummy_cols(df, final_df, bim_var, 'dum')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_stacktest(stacktest_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    #I HAVEN'T PUT IN POLLUTANT_CODES\n",
    "    cat_var = ['AIR_STACK_TEST_STATUS_CODE', 'COMP_MONITOR_TYPE_CODE']\n",
    "    \n",
    "    df = stacktest_df[stacktest_df['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    df = df[df['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 0 (string) --> THIS ONLY APPLIES TO THE BIM_VAR!!\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize --> Not really needed here \n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.ACTUAL_END_DATE_year\n",
    "    \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def process_formalact(formalact_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    cat_var = ['ENF_TYPE_CODE']\n",
    "    cont_var = ['PENALTY_AMOUNT']\n",
    "    \n",
    "    df = formalact_df[formalact_df['SETTLEMENT_ENTERED_DATE_year'] >= start_year]\n",
    "    df = df[df['SETTLEMENT_ENTERED_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 0 (string) --> No need for this but ill leave it here\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize --> Not really needed here \n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.SETTLEMENT_ENTERED_DATE_year\n",
    "    \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "\n",
    "    sum_df = formalact_df.groupby('PGM_SYS_ID')[cont_var[0]].sum().to_frame()\n",
    "    final_df = pd.merge(final_df, sum_df, on = 'PGM_SYS_ID')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_informalact(informalact_df, start_year, end_year):\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    cat_var = ['ENF_TYPE_CODE']\n",
    "    \n",
    "    df = informalact_df[informalact_df['ACHIEVED_DATE_year'] >= start_year]\n",
    "    df = df[df['ACHIEVED_DATE_year'] <= end_year]\n",
    "    \n",
    "    ## Replace NaN with 0 (string) --> No need for this but ill leave it here\n",
    "    nan = df.columns[df.isnull().any()].tolist()\n",
    "    values = ['None']*len(nan)\n",
    "    replace_with_value(df, nan, values)\n",
    "    \n",
    "    ## Dummitize --> Not really needed here \n",
    "    \n",
    "    df['id_+_date'] = df.PGM_SYS_ID +'_'+ df.ACHIEVED_DATE_year\n",
    "    \n",
    "    final_df = aggr_dummy_cols(df, final_df, cat_var, 'cat')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def process_noninspectHPV(violhist, fce, start_year, end_year):\n",
    "    #removing FRVs\n",
    "    violhist = violhist[violhist.ENF_RESPONSE_POLICY_CODE != 'FRV']\n",
    "    violhist = violhist[violhist['HPV_DAYZERO_DATE_year'] >= start_year]\n",
    "    violhist = violhist[violhist['HPV_DAYZERO_DATE_year'] <= end_year]\n",
    "    \n",
    "    #for fce\n",
    "    fce = fce[fce['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    fce = fce[fce['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    #Steps before merge\n",
    "    violhist['year'] = violhist['HPV_DAYZERO_DATE_year']\n",
    "    merged_hpv_fce = pd.merge(violhist, fce, how='left', left_on=['PGM_SYS_ID', 'HPV_DAYZERO_DATE'], right_on=['PGM_SYS_ID','ACTUAL_END_DATE'])\n",
    "    # Find violations that resulted from something other than an inspection \n",
    "    viol_by_other = merged_hpv_fce\n",
    "    viol_by_other.COMP_MONITOR_TYPE_CODE.fillna(0, inplace=True)   #this will be NaN because it was not inspected\n",
    "    viol_by_other = viol_by_other[viol_by_other['COMP_MONITOR_TYPE_CODE'] == 0]  #violations not resulting from inspections\n",
    "    # Get the columns needed\n",
    "    viol_by_other = viol_by_other[['PGM_SYS_ID','year']]\n",
    "    viol_other_year = viol_by_other.groupby(['PGM_SYS_ID','year']).size().reset_index() # to get count of HPV by year\n",
    "    violhist2 = violhist[['PGM_SYS_ID','year']]\n",
    "    # Outer merge\n",
    "    merged_viols = pd.merge(violhist2,viol_other_year, how = 'outer', on = ['PGM_SYS_ID','year'])\n",
    "    merged_viols.rename(columns={'year': 'Year', 0:'NonInspection_HPV_Count'}, inplace=True)\n",
    "    merged_viols.NonInspection_HPV_Count.fillna(0, inplace=True)\n",
    "    merged_viols = merged_viols.dropna(axis=0)\n",
    "    merged_viols['id_+_date'] = merged_viols.PGM_SYS_ID +'_'+ merged_viols.Year\n",
    "    merged_viols.drop(['PGM_SYS_ID', 'Year'], axis = 1, inplace = True)\n",
    "    \n",
    "    return merged_viols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### SHOULD BE WITHIN CLEANING.PY ###\n",
    "## FROM PREPROCESSING ##\n",
    "#interest_var = ['PGM_SYS_ID','ACTIVITY_ID','AGENCY_TYPE_DESC','STATE_CODE','AIR_LCON_CODE','COMP_DETERMINATION_UID','ENF_RESPONSE_POLICY_CODE','PROGRAM_CODES']\n",
    "def replace_with_value(data_file, variables, values):\n",
    "    '''\n",
    "    '''\n",
    "    for variable in variables:\n",
    "        value = values[variables.index(variable)]\n",
    "        data_file[variable] = data_file[variable].fillna(value)\n",
    "\n",
    "def convert_to_datetime(series_row, date_format):\n",
    "    if str(series_row) == 'nan':\n",
    "        return float('nan')\n",
    "    return dt.strptime(series_row, date_format)\n",
    "\n",
    "def convert_to_year(series_row):\n",
    "    if str(series_row) == 'NaT' or str(series_row)== 'nan':\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return str(series_row.year)\n",
    "\n",
    "def to_date_time(df, date_format, date_col):\n",
    "    #add datetime column\n",
    "    df[date_col] = df[date_col].apply(convert_to_datetime, date_format=date_format)\n",
    "    df[date_col+'_year'] = df[date_col].apply(convert_to_year)\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_to_month(series_row):\n",
    "    if str(series_row) == 'NaT' or str(series_row)== 'nan':\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return str(series_row.month)\n",
    "\n",
    "def get_month_year_col(df, date_column, date_format):\n",
    "    df[date_column+'_datetime'] = df[date_column].apply(convert_to_datetime, date_format=date_format)\n",
    "    df[date_column+'_month'] = df[date_column+'_datetime'].apply(convert_to_month)\n",
    "    df[date_column+'_year'] = df[date_column+'_datetime'].apply(convert_to_year)\n",
    "    return df\n",
    "\n",
    "def filter_date(df, date_format, date_col, start=None, end=None):\n",
    "    df = to_date_time(df, date_format, date_col)\n",
    "    \n",
    "    \n",
    "    if start:\n",
    "        timestart = dt.strptime(start,\"%Y/%m/%d\")\n",
    "        #print(start)\n",
    "        df = df[df[date_col] >= timestart ]\n",
    "        #print(df.head())\n",
    "    if end:\n",
    "        timeend = dt.strptime(end,\"%Y/%m/%d\")\n",
    "        #print(end)\n",
    "        df = df[df[date_col] <= timeend ]\n",
    "        #print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_col(df, fac_id, features, date_col):\n",
    "    #filter needed\n",
    "    df = df[[fac_id] + [date_col] + [date_col+'_year'] + features]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## FROM PREPROCESSING, MODIFIED ##\n",
    "def add_dummy(df, variable_list, sep_char = None, drop_one=False, drop_original=False):\n",
    "    '''\n",
    "    Input: \n",
    "        - df: pandas dataframe\n",
    "        - variable_list: a list of variables to dummitize\n",
    "        - drop_one: whether to drop first dummy\n",
    "        - drop_original: whether to drop original categorical variable\n",
    "    Output: dataframe with tht dummy variables added\n",
    "    '''\n",
    "    for variable in variable_list:\n",
    "        if sep_char:\n",
    "            df_dummy = df[variable].str.get_dummies(sep=sep_char)\n",
    "            df_dummy.columns = [variable+ '_' +str(col) for col in df_dummy.columns]\n",
    "\n",
    "        else:\n",
    "            df_dummy = pd.get_dummies(df[variable], drop_first=drop_one, prefix = variable)\n",
    "        \n",
    "        df = pd.concat([df, df_dummy], axis=1)\n",
    "        if drop_original:\n",
    "            df = df.drop(variable, 1)\n",
    "    return (df, df_dummy.columns)\n",
    "\n",
    "\n",
    "def aggr_dummy_cols(df, final_df, colnames, mode = None):\n",
    "    for col in colnames:\n",
    "        \n",
    "        cross = pd.crosstab(df['id_+_date'], columns=df[col])\n",
    "        \n",
    "        if mode == 'cat':\n",
    "            cross.columns = [cross.columns.name+ '_' +str(col) for col in cross.columns]\n",
    "        \n",
    "        elif mode == 'dum':\n",
    "            cross = cross.drop(0, axis = 1)\n",
    "            cross.columns = [cross.columns.name for col in cross.columns]\n",
    "        \n",
    "        else:\n",
    "            cross.columns = [cross.columns.name for col in cross.columns]\n",
    "            \n",
    "        cross.columns.name = None\n",
    "        cross.reset_index(inplace=True)\n",
    "        \n",
    "        \n",
    "        if final_df.empty:\n",
    "            final_df = final_df.append(cross)\n",
    "        else:\n",
    "            final_df = pd.merge(final_df, cross, how = 'left', on = 'id_+_date')\n",
    "            \n",
    "    return final_df\n",
    "\n",
    "## FROM ULAB PIPELINE ##\n",
    "\n",
    "def generate_continous_variable(data_file, variable_list):\n",
    "    '''\n",
    "    function that can take a categorical variable and create \n",
    "    binary variables from it\n",
    "    '''\n",
    "    for variable in variable_list:\n",
    "        list_values = list(data_file.groupby(variable).groups.keys())\n",
    "        for i,value in enumerate(list_values):\n",
    "            data_file[variable] = data_file[variable].replace(value,i)\n",
    "\n",
    "    return data_file \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## CONFIG DATA ##\n",
    "START_DATE= '2007/01/01'\n",
    "END_DATE = '2016/12/31'\n",
    "fac_id = 'PGM_SYS_ID'\n",
    "\n",
    "VIOLATION = 'violation'\n",
    "INSPECTION = 'inspection'\n",
    "STACKTEST = 'stacktest'\n",
    "TITLEV = 'titlev'\n",
    "FORMALACT = 'formalact'\n",
    "INFORMALACT = 'informalact'\n",
    "\n",
    "\n",
    "### DON'T FORGET TO CHANGE THE DATA_FILE!!! ###\n",
    "df_dict ={'violation': {'data_file': 'ICIS-AIR_downloads/ICIS-AIR_VIOLATION_HISTORY.csv',\n",
    "                        'interest_var': ['AGENCY_TYPE_DESC','AIR_LCON_CODE','ENF_RESPONSE_POLICY_CODE','POLLUTANT_CODES','PROGRAM_CODES','HPV_RESOLVED_DATE'],\n",
    "                         'date_col': 'HPV_DAYZERO_DATE',\n",
    "                       'date_format':'%m-%d-%Y'},\n",
    "          \n",
    "           'inspection': {'data_file': 'ICIS-AIR_downloads/ICIS-AIR_FCES_PCES.csv',\n",
    "                          'interest_var': ['STATE_EPA_FLAG','COMP_MONITOR_TYPE_CODE','PROGRAM_CODES'],\n",
    "                          'date_col': 'ACTUAL_END_DATE',\n",
    "                          'date_format':'%m-%d-%Y'},\n",
    "          \n",
    "           'stacktest': {'data_file':'ICIS-AIR_downloads/ICIS-AIR_STACK_TESTS.csv',\n",
    "                         'interest_var':['COMP_MONITOR_TYPE_CODE','POLLUTANT_CODES','AIR_STACK_TEST_STATUS_CODE'],\n",
    "                        'date_col': 'ACTUAL_END_DATE',\n",
    "                        'date_format':'%m/%d/%Y'},\n",
    "          \n",
    "           'titlev':{'data_file': 'ICIS-AIR_downloads/ICIS-AIR_TITLEV_CERTS.csv',\n",
    "                     'interest_var':['COMP_MONITOR_TYPE_CODE','FACILITY_RPT_DEVIATION_FLAG'],\n",
    "                        'date_col': 'ACTUAL_END_DATE',\n",
    "                    'date_format':'%m/%d/%Y'},\n",
    "          \n",
    "           'formalact':{'data_file': 'ICIS-AIR_downloads/ICIS-AIR_FORMAL_ACTIONS.csv',\n",
    "                       'interest_var':['ENF_TYPE_CODE','PENALTY_AMOUNT'],\n",
    "                        'date_col': 'SETTLEMENT_ENTERED_DATE',\n",
    "                       'date_format':'%m/%d/%Y'},\n",
    "          \n",
    "           'informalact':{'data_file': 'ICIS-AIR_downloads/ICIS-AIR_INFORMAL_ACTIONS.csv',\n",
    "                          'interest_var':['ENF_TYPE_CODE'],\n",
    "                        'date_col': 'ACHIEVED_DATE',\n",
    "                         'date_format':'%m/%d/%Y'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## READ INITIAL FILTERED FILE ##\n",
    "violation = general_read_file(df_dict, VIOLATION, START_DATE, END_DATE)\n",
    "inspection = general_read_file(df_dict, INSPECTION, START_DATE, END_DATE)\n",
    "titlev = general_read_file(df_dict, TITLEV, START_DATE, END_DATE)\n",
    "stacktest = general_read_file(df_dict, STACKTEST, START_DATE, END_DATE)\n",
    "formalact = general_read_file(df_dict, FORMALACT, START_DATE, END_DATE)\n",
    "informalact = general_read_file(df_dict, INFORMALACT, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_features(start_date, end_date):\n",
    "    #violation, inspection, titlev, stacktest, formalact, informalact = general_read_file(df_dict, START_DATE, END_DATE)\n",
    "    \n",
    "    violation_df = process_violation(violation, start_date, end_date)\n",
    "    inspection_df = process_titlev(titlev, start_date, end_date)\n",
    "    stacktest_df = process_stacktest(stacktest, start_date, end_date)\n",
    "    formalact_df = process_formalact(formalact, start_date, end_date)\n",
    "    informalact_df = process_informalact(informalact, start_date, end_date)\n",
    "    noninspectHPV_df = process_noninspectHPV(violation, inspection, start_date, end_date)\n",
    "    \n",
    "    final_df = pd.merge(inspection_df, violation_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "    final_df = pd.merge(final_df, stacktest_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "    final_df = pd.merge(final_df, formalact_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "    final_df = pd.merge(final_df, informalact_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "    final_df = pd.merge(final_df, noninspectHPV_df, how = 'left', right_on = [\"id_+_date\"], left_on = [\"id_+_date\"])\n",
    "\n",
    "    re_separate = r'(.[^_]*)_(.*)'\n",
    "    sep = lambda x: pd.Series([i for i in re.split(re_separate,x)])\n",
    "    final_id_year = final_df['id_+_date'].apply(sep)\n",
    "    final_df =pd.concat([final_id_year.rename(columns={1:'PGM_SYS_ID',2:'HPV_DAYZERO_DATE_year'}), final_df], axis=1)\n",
    "    final_df.drop([0, 3, 'id_+_date'], axis = 1, inplace = True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56246, 173)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for feature generation\n",
    "start_date = '2009'\n",
    "end_date = '2012'\n",
    "feat = generate_features(start_date, end_date)\n",
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### FAILED LABEL #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_label(violhist, fce, start_year, end_year):\n",
    "    #removing FRVs\n",
    "    violhist = violhist[violhist.ENF_RESPONSE_POLICY_CODE != 'FRV']\n",
    "    violhist = violhist[violhist['HPV_DAYZERO_DATE_year'] >= start_year]\n",
    "    violhist = violhist[violhist['HPV_DAYZERO_DATE_year'] <= end_year]\n",
    "    \n",
    "    #for fce\n",
    "    fce = fce[fce['ACTUAL_END_DATE_year'] >= start_year]\n",
    "    fce = fce[fce['ACTUAL_END_DATE_year'] <= end_year]\n",
    "    \n",
    "    merged_hpv_fce = pd.merge(violhist, fce, how='right', left_on=['PGM_SYS_ID', 'HPV_DAYZERO_DATE'], right_on=['PGM_SYS_ID','ACTUAL_END_DATE'])\n",
    "    \n",
    "    #finding 0's\n",
    "    '''\n",
    "    non_viol = merged_hpv_fce[merged_hpv_fce.HPV_DAYZERO_DATE.isnull()]\n",
    "    non_viol['is_violation'] = 0\n",
    "    '''\n",
    "    non_viol = merged_hpv_fce\n",
    "    non_viol.HPV_DAYZERO_DATE.fillna(0, inplace=True)\n",
    "    #non_viol = non_viol.filter(['PGM_SYS_ID', 'ACTUAL_END_DATE', 'is_violation', 'ACTUAL_END_DATE_year'], axis = 1)\n",
    "    non_viol = non_viol[['PGM_SYS_ID', 'ACTUAL_END_DATE', 'ACTUAL_END_DATE_year']]\n",
    "    non_viol.rename(columns={'ACTUAL_END_DATE': 'HPV_DAYZERO_DATE', 'ACTUAL_END_DATE_year': \"HPV_DAYZERO_DATE_year\" }, inplace=True)\n",
    "    #finding 1's\n",
    "    #viol = merged_hpv_fce[merged_hpv_fce.HPV_DAYZERO_DATE.notnull()]\n",
    "    viol = merged_hpv_fce\n",
    "    viol.HPV_DAYZERO_DATE.fillna(0, inplace=True)\n",
    "    #viol['is_violation'] = 1\n",
    "    #viol = viol.filter(['PGM_SYS_ID', 'HPV_DAYZERO_DATE', 'is_violation', 'HPV_DAYZERO_DATE_year'], axis = 1)\n",
    "    viol = viol[['PGM_SYS_ID', 'HPV_DAYZERO_DATE', 'HPV_DAYZERO_DATE_year']]\n",
    "    output = pd.concat([viol,non_viol])\n",
    "    #output['id_+_date'] = output.PGM_SYS_ID +'_'+ output.HPV_DAYZERO_DATE_year\n",
    "    #output.drop(['PGM_SYS_ID', 'HPV_DAYZERO_DATE', 'HPV_DAYZERO_DATE_year'], axis = 1, inplace = True)\n",
    "    #output = output.groupby('id_+_date').size().reset_index()\n",
    "    #output.rename(columns = {0:\"outcome\"}, inplace = True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2834: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "out = generate_label(violation, inspection, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output = out.groupby(['PGM_SYS_ID', 'HPV_DAYZERO_DATE_year' ]).size().reset_index()\n",
    "output.rename(columns = {0:\"outcome\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PGM_SYS_ID</th>\n",
       "      <th>HPV_DAYZERO_DATE_year</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PGM_SYS_ID, HPV_DAYZERO_DATE_year, outcome]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[output.outcome == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
